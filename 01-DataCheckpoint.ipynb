{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
    "\n",
    "Scoring: Out of 10 points\n",
    "\n",
    "- Each Developing  => -2 pts\n",
    "- Each Unsatisfactory/Missing => -4 pts\n",
    "  - until the score is \n",
    "\n",
    "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
    "\n",
    "\n",
    "|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n",
    "| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n",
    "| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with your team list and their contributions. Note that this will change over the course of the checkpoints\n",
    "\n",
    "This is a modified [CRediT taxonomy of contributions](https://credit.niso.org). For each group member please list how they contributed to this project using these terms:\n",
    "> Analysis, Background research, Conceptualization, Data curation, Experimental investigation, Methodology, Project administration, Software, Visualization, Writing – original draft, Writing – review & editing\n",
    "\n",
    "Example team list and credits:\n",
    "- Alice Anderson: Conceptualization, Data curation, Methodology, Writing - original draft\n",
    "- Bob Barker:  Analysis, Software, Visualization\n",
    "- Charlie Chang: Project administration, Software, Writing - review & editing\n",
    "- Dani Delgado: Analysis, Background research, Visualization, Writing - original draft"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was there a significant difference between crime rates in California college cities/towns compared to the crime rates of their local county in 2024? Does a correlation exists between crime rates and whether a city or town is a college city or town?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict that there will be a significant difference between the crime rates in the college cities/town and their local county, especially the property crimes. Through our background research, we found a study published in the American Journal of Police indicating that there were significantly higher property crime rates yet lower crime rates in the university area compared to the cities/county due to college students being easier targets, and the findings were consistent with other studies such as from University of South Florida. Our null hypothesis is there is no significant difference in the aforementioned above and our alternative hypothesis is that there is a significant difference in the aforementioned above.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with descriptions of your actual datasets.\n",
    "\n",
    "For each dataset include the following information\n",
    "- Dataset #1\n",
    "  - Dataset Name:\n",
    "  - Link to the dataset:\n",
    "  - Number of observations:\n",
    "  - Number of variables:\n",
    "  - Description of the variables most relevant to this project\n",
    "  - Descriptions of any shortcomings this dataset has with repsect to the project\n",
    "\n",
    "\n",
    "- IPEDS Dataset #1\n",
    "  - Datset Name: EF2023-2024.csv\n",
    "  - Link to the dataset: https://drive.google.com/file/d/1PB844Stx1ytIB0PR2eBrNeKv3zGwj43U/view?usp=sharing\n",
    "  - Number of observations: 42248\n",
    "  - Number of variables: 11\n",
    "    - UNITID: Unique IPEDS unique identifier code for each school\n",
    "    - EFLEVEL: Enrollment level category\n",
    "      {\n",
    "        10 : \"All Level of Students\",\n",
    "        20 : \"All Students, Undergraduates Total\",\n",
    "        30 : \"All students, Undergraduate, Degree/certificate-seeking total\",\n",
    "        31 : \"All students, Undergraduate, Degree/certificate-seeking, First-time\",\n",
    "        34 : \"All students, Undergraduate, Other degree/certificate-seeking\",\n",
    "        35 : \"All students, Undergraduate, Other degree/certificatee-seeking, Transfer-ins\",\n",
    "        36 : \"All students, Undergraduate, Other degree/certificatee-seeking, Continuing\",\n",
    "        40 : \"All students, Undergraduate, Non-degree/certificate-seeking\",\n",
    "        50 : \"All students, Graduate \",\n",
    "      }\n",
    "    - EFTOTAL : Total number of students enrolled\n",
    "  - This dataset is dependent on another dataset from IPEDS, where it matches IPEDS school identifiers with\n",
    "    school names, city locations, and state abbreviations.\n",
    "\n",
    "- IPEDS Dataset #2\n",
    "  - Dataset Name: HD2023-2024.csv\n",
    "  - Link to the dataset: https://drive.google.com/file/d/1iYw43ccnUyXxZUVEt5IZjQQflbtrqkIp/view?usp=sharing\n",
    "  - Number of observations: 6164\n",
    "  - Number of variables: 72\n",
    "    - UNITID: Unique IPEDS unique identifier code for each school\n",
    "    - INSTNM: Institution name\n",
    "    - ADDR: Address (i.e \"2025 Yukon Drive, Suite 202 Butrovich Building\")\n",
    "    - CITY: City name\n",
    "    - STABBR: State abbreviation\n",
    "    - ZIP: Zip code\n",
    "  - This dataset will have to be used in conjunction with IPEDS Dataset #1 named \"EF2023-2024.csv\" to match\n",
    "    enrollment information with institution metadata. We plan to merge the two datasets on UNITID to use both\n",
    "    in conjunction.\n",
    "\n",
    "Each dataset deserves either a set of bullet points as above or a few sentences if you prefer that method.\n",
    "\n",
    "If you plan to use multiple datasets, add a few sentences about how you plan to combine these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Download Progress:  50%|█████     | 1/2 [00:01<00:01,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: EF2023-2024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Download Progress: 100%|██████████| 2/2 [00:04<00:00,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: HD2023-2024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "# %pip install requests tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') # this tells python where to look for modules to import\n",
    "\n",
    "import get_data # this is where we get the function we need to download data\n",
    "\n",
    "# replace the urls and filenames in this list with your actual datafiles\n",
    "# yes you can use Google drive share links or whatever\n",
    "# format is a list of dictionaries; \n",
    "# each dict has keys of \n",
    "#   'url' where the resource is located\n",
    "#   'filename' for the local filename where it will be stored \n",
    "datafiles = [\n",
    "    { \n",
    "        'url': 'https://drive.google.com/uc?export=download&id=1PB844Stx1ytIB0PR2eBrNeKv3zGwj43U', \n",
    "        'filename':'EF2023-2024.csv'\n",
    "    },\n",
    "    { \n",
    "        'url': 'https://drive.google.com/uc?export=download&id=1iYw43ccnUyXxZUVEt5IZjQQflbtrqkIp', \n",
    "        'filename':'HD2023-2024.csv'\n",
    "    }\n",
    "]\n",
    "\n",
    "get_data.get_raw(datafiles,destination_directory='data/00-raw/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FBI NIBRS California 2024 —> County Offense Counts & Rates\n",
    "\n",
    "Instructions: \n",
    "1. Change the header from Dataset #1 to something more descriptive of the dataset\n",
    "2. Write a few paragraphs about this dataset. Make sure to cover\n",
    "   1. Describe the important metrics, what units they are in, and give some sense of what they mean.  For example \"Fasting blood glucose in units of mg glucose per deciliter of blood.  Normal values for healthy individuals range from 70 to 100 mg/dL.  Values 100-125 are prediabetic and values >125mg/dL indicate diabetes. Values <70 indicate hypoglycemia. Fasting idicates the patient hasn't eaten in the last 8 hours.  If blood glucose is >250 or <50 at any time (regardless of the time of last meal) the patient's life may be in immediate danger\"\n",
    "   2. If there are any major concerns with the dataset, describe them. For example \"Dataset is composed of people who are serious enough about eating healthy that they voluntarily downloaded an app dedicated to tracking their eating patterns. This sample is likely biased because of that self-selection. These people own smartphones and may be healthier and may have more disposable income than the average person.  Those who voluntarily log conscientiously and for long amounts of time are also likely even more interested in health than those who download the app and only log a bit before getting tired of it\"\n",
    "3. Use the cell below to \n",
    "    1. load the dataset \n",
    "    2. make the dataset tidy or demonstrate that it was already tidy\n",
    "    3. demonstrate the size of the dataset\n",
    "    4. find out how much data is missing, where its missing, and if its missing at random or seems to have any systematic relationships in its missingness\n",
    "    5. find and flag any outliers or suspicious entries\n",
    "    6. clean the data or demonstrate that it was already clean.  You may choose how to deal with missingness (dropna of fillna... how='any' or 'all') and you should justify your choice in some way\n",
    "    7. You will load raw data from `data/00-raw/`, you will (optionally) write intermediate stages of your work to `data/01-interim` and you will write the final fully wrangled version of your data to `data/02-processed`\n",
    "4. Optionally you can also show some summary statistics for variables that you think are important to the project\n",
    "5. Feel free to add more cells here if that's helpful for you\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describing the important metrics:\n",
    "This dataset comes from the FBI’s NIBRS incident-based reporting for California in 2024. After joining the offense table with offense type and joining incidents to agencies/counties, each offense record can be counted and summarized by county. Our important metrics are: offense_rows, offense_name and the offense_category_name it fits into like Aggrevated assult would be categorized as a violent crime, and offences_per_100k.\n",
    "\n",
    "offense_rows: The total number of reported offense records in a county for a given offense type. This represents the crimes reported to law enforcement agencies in each county. For example, if Los Angeles has 5,000 theft offenses, that means 5,000 separate theft incidents were officially recorded by law enforcement agencies in that county during 2024.\n",
    "\n",
    "offenses_per_100k: This metric helps standarde the offense counts per 100,000 residents using county population data. The rate per 100,000 helps compare counties of different sizes more fairly than raw counts. For instance, a small county with 50,000 people reporting 100 burglaries has a rate of 200 per 100k, which can be directly compared to a large county with 1 million people reporting 3,000 burglaries, so 300 per 100k.\n",
    "\n",
    "offense_name and offense_category_name: These classify crimes into specific types and broader categories like \"Violent Crimes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['incident_id', 'agency_id', 'data_year'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     24\u001b[39m county_pop = (ag.groupby(\u001b[33m\"\u001b[39m\u001b[33mcounty_name\u001b[39m\u001b[33m\"\u001b[39m, as_index=\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[33m\"\u001b[39m\u001b[33mpopulation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     25\u001b[39m                 .sum()\n\u001b[32m     26\u001b[39m                 .rename(columns={\u001b[33m\"\u001b[39m\u001b[33mpopulation\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcounty_population\u001b[39m\u001b[33m\"\u001b[39m}))\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# incident -> agency -> county\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m inc_ag = \u001b[43minc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mincident_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43magency_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata_year\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.merge(\n\u001b[32m     30\u001b[39m     ag[[\u001b[33m\"\u001b[39m\u001b[33magency_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdata_year\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcounty_name\u001b[39m\u001b[33m\"\u001b[39m]],\n\u001b[32m     31\u001b[39m     on=[\u001b[33m\"\u001b[39m\u001b[33magency_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdata_year\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     32\u001b[39m     how=\u001b[33m\"\u001b[39m\u001b[33minner\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     33\u001b[39m )\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# offense -> offense name/category -> county\u001b[39;00m\n\u001b[32m     36\u001b[39m off_named = off.merge(\n\u001b[32m     37\u001b[39m     ot[[\u001b[33m\"\u001b[39m\u001b[33moffense_code\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33moffense_name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33moffense_category_name\u001b[39m\u001b[33m\"\u001b[39m]],\n\u001b[32m     38\u001b[39m     on=\u001b[33m\"\u001b[39m\u001b[33moffense_code\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     how=\u001b[33m\"\u001b[39m\u001b[33minner\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     44\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/COGS108_FA25/lib/python3.11/site-packages/pandas/core/frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/COGS108_FA25/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/COGS108_FA25/lib/python3.11/site-packages/pandas/core/indexes/base.py:6249\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6252\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['incident_id', 'agency_id', 'data_year'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "DATA_DIR = \"data/00-raw/CA-2024\"\n",
    "OUT_DIR = \"data/02-processed\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "ag  = pd.read_csv(f\"{DATA_DIR}/agencies.csv\", encoding=\"latin1\", low_memory=False)\n",
    "off = pd.read_csv(f\"{DATA_DIR}/NIBRS_OFFENSE.csv\", low_memory=False)\n",
    "ot  = pd.read_csv(f\"{DATA_DIR}/NIBRS_OFFENSE_TYPE.csv\", low_memory=False)\n",
    "inc = pd.read_csv(f\"{DATA_DIR}/NIBRS_incident.csv\", low_memory=False)\n",
    "\n",
    "# filter to CA + 2024\n",
    "ag = ag[\n",
    "    (ag[\"data_year\"] == 2024) &\n",
    "    (ag[\"state_postal_abbr\"] == \"CA\") &\n",
    "    (ag[\"publishable_flag\"] == \"Y\")\n",
    "]\n",
    "off = off[off[\"data_year\"] == 2024]\n",
    "inc = inc[inc[\"data_year\"] == 2024]\n",
    "\n",
    "# county population\n",
    "county_pop = (ag.groupby(\"county_name\", as_index=False)[\"population\"]\n",
    "                .sum()\n",
    "                .rename(columns={\"population\": \"county_population\"}))\n",
    "\n",
    "# incident -> agency -> county\n",
    "inc_ag = inc[[\"incident_id\", \"agency_id\", \"data_year\"]].merge(\n",
    "    ag[[\"agency_id\", \"data_year\", \"county_name\"]],\n",
    "    on=[\"agency_id\", \"data_year\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# offense -> offense name/category -> county\n",
    "off_named = off.merge(\n",
    "    ot[[\"offense_code\", \"offense_name\", \"offense_category_name\"]],\n",
    "    on=\"offense_code\",\n",
    "    how=\"left\"\n",
    ").merge(\n",
    "    inc_ag[[\"incident_id\", \"county_name\"]],\n",
    "    on=\"incident_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "missing = off_named.isna().mean().sort_values(ascending=False)\n",
    "display(missing.head(15))\n",
    "\n",
    "# counts + rates\n",
    "counts = (off_named.groupby([\"county_name\", \"offense_category_name\", \"offense_name\"], as_index=False)\n",
    "                  .size()\n",
    "                  .rename(columns={\"size\": \"offense_rows\"}))\n",
    "\n",
    "final = counts.merge(county_pop, on=\"county_name\", how=\"left\")\n",
    "final[\"offenses_per_100k\"] = final[\"offense_rows\"] / final[\"county_population\"] * 100000\n",
    "\n",
    "display(final.sort_values(\"offense_rows\", ascending=False).head(20))\n",
    "\n",
    "final.to_csv(f\"{OUT_DIR}/ca2024_county_offense_rates.csv\", index=False)\n",
    "county_pop.to_csv(f\"{OUT_DIR}/ca2024_county_population.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\",\n",
    "      f\"{OUT_DIR}/ca2024_county_offense_rates.csv\",\n",
    "      \"and\",\n",
    "      f\"{OUT_DIR}/ca2024_county_population.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IPEDS Dataset #1 (Enrollment Information) \n",
    "\n",
    "The EF2023-2024 dataset is from IPEDS (Integrated Postsecondary Education Data System) enrollment survey for Fall 2023. Each row represents a single instution identifiable by a unique UNITID. The columns EFLEVEL contains two digit codes representing enrollment categories for that row. For example, a value of 10 represents total enrollment across all levels of students, 20 represents total undergraduate enrollment, and 50 represents total graduate enrollment. These three enrollment categories will likely be the primary focus of our analysis. The main variable of interest is EFTOTAL, which represents the number of enrolled students for the specified category at each institution. This will serve as a proxy for institution size.\n",
    "\n",
    "One limitation is this datasets timing. The surey was conducted at the beginning of the 2023-2024 school year, meaning that it does not represent a full year's average. Because some of our other datasets reprepsent conditions from 2024, there is a slight misalignment in timing. Additionaly, this dataset doesn't account for students dropping out, transferring, or unenrolling later in the year. Essentially theres a very rare chance instutitonal shutdown or enrollment shifts could fruther lter the student population. Thus, the accuracy when correlating enrollment with variables such as crime rates or regional population might be impacted. Additionally, enrollment data is self-reported by institutions, and differences in how schools classify full-time and part-time students could introduce further inconsistencies. Finally, the dataset only includes institutions that participate in IPEDS surveys, meaning some  institutions may be omitted.\n",
    "\n",
    "### IPEDS Dataset #2 (Instution Metadata)\n",
    "\n",
    "The HD2023–2024 dataset is from IPEDS (Integrated Postsecondary Education Data System) and contains institutional level metadata for the 2023–2024 academic school year. Each row represents a single postsecondary institution and is uniquely identified by a UNITID. Key variables include INSTNM (institution name), ADDR (street address), CITY, STABBR (state abbreviation), and ZIP (postal code). These variables describe where each institution is located and allow for geographic grouping or regional analysis. \n",
    "\n",
    "This dataset will be used in conjunction with the EF2023–2024 enrollment dataset. By merging both datasets on the shared UNITID variable, we can combine institutional metadata with enrollment counts. This allows enrollment size (from EF2023–2024) to be linked with institutional locational characteristcs. We will likely analyze with different enrollment sizes assocaited with its zip code or city region.\n",
    "\n",
    "One limitation of the HD2023–2024 dataset is that it represents institutional characteristics at a specific point in time during the 2023–2024 academic year. Institutional attributes may change over time (for example, a school closing, merging, or changing classification), and these updates may not immediately reflect broader structural shifts. Additionally, as with other IPEDS data, the information is self-reported by institutions, which may introduce minor inconsistencies in how classifications are recorded. Finally, because IPEDS includes only institutions that participate in federal reporting, certain non-participating or very small institutions may be excluded. Despite these limitations, the dataset provides comprehensive institutional-level metadata that is essential for linking enrollment data to meaningful contextual variables.\n",
    "\n",
    "\n",
    "\n",
    "   3. Use the cell below to \n",
    "    1. load the dataset \n",
    "    2. make the dataset tidy or demonstrate that it was already tidy\n",
    "    3. demonstrate the size of the dataset\n",
    "    4. find out how much data is missing, where its missing, and if its missing at random or seems to have any systematic relationships in its missingness\n",
    "    5. find and flag any outliers or suspicious entries\n",
    "    6. clean the data or demonstrate that it was already clean.  You may choose how to deal with missingness (dropna of fillna... how='any' or 'all') and you should justify your choice in some way\n",
    "    7. You will load raw data from `data/00-raw/`, you will (optionally) write intermediate stages of your work to `data/01-interim` and you will write the final fully wrangled version of your data to `data/02-processed`\n",
    "4. Optionally you can also show some summary statistics for variables that you think are important to the project\n",
    "5. Feel free to add more cells here if that's helpful for you\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNITID     0\n",
      "EFLEVEL    0\n",
      "EFTOTAL    0\n",
      "dtype: int64\n",
      "UNITID      0\n",
      "INSTNM      0\n",
      "ADDR        0\n",
      "CITY        0\n",
      "STABBR      0\n",
      "ZIP         0\n",
      "COUNTYNM    0\n",
      "dtype: int64\n",
      "\n",
      "Size of Datasets\n",
      "ef23_24: (42247, 3)\n",
      "hd23_24: (6163, 7)\n",
      "\n",
      "4. No data is missing\n",
      "\n",
      "5. No suspicious entries\n",
      "\n",
      "6. Data looks clean\n",
      "   UNITID  EFLEVEL  EFTOTAL\n",
      "0  100654       10     6614\n",
      "1  100663       10    21160\n",
      "2  100690       10      636\n",
      "3  100706       10     8743\n",
      "4  100724       10     3870\n",
      "   UNITID                                           INSTNM  \\\n",
      "0  103529  University of Alaska System of Higher Education   \n",
      "1  103741                      Empire Beauty School-Tucson   \n",
      "2  103893                 Carrington College-Phoenix North   \n",
      "3  103909                          Carrington College-Mesa   \n",
      "4  103927                        Carrington College-Tucson   \n",
      "\n",
      "                                             ADDR       CITY STABBR  \\\n",
      "0  2025 Yukon Drive, Suite 202 Butrovich Building  Fairbanks     AK   \n",
      "1                            3030 E Speedway Blvd     Tucson     AZ   \n",
      "2                            2149 W Dunlap Avenue    Phoenix     AZ   \n",
      "3                       1001 West Southern Avenue       Mesa     AZ   \n",
      "4                         201 North Bonita Avenue     Tucson     AZ   \n",
      "\n",
      "          ZIP                      COUNTYNM  \n",
      "0  99775-5000  Fairbanks North Star Borough  \n",
      "1       85716                   Pima County  \n",
      "2  85051-4063               Maricopa County  \n",
      "3  85210-5005               Maricopa County  \n",
      "4  85745-2974                   Pima County  \n"
     ]
    }
   ],
   "source": [
    "# 1. Load the dataset\n",
    "ef23_24 = pd.read_csv(\"data/00-raw/EF2023-2024.csv\")\n",
    "hd23_24 = pd.read_csv(\"data/00-raw/HD2023-2024.csv\")\n",
    "\n",
    "\n",
    "# 2. Dataset already tidy\n",
    "\n",
    "# Counts the number of na values each column,\n",
    "# no cleaning required because there are \n",
    "# no na values in UNITID, EFLEVEL, and EFTOTAL\n",
    "ef23_24_important_cols = ef23_24[\n",
    "   [\"UNITID\", \"EFLEVEL\", \"EFTOTAL\"]\n",
    "]\n",
    "print(ef23_24_important_cols.isna().sum())\n",
    "\n",
    "# Counts the number of na values each column,\n",
    "# no cleaning required because there are \n",
    "# no na values in UNITID, INSTNM, ADDR, CITY, STABBR, and ZIP\n",
    "hd23_24_important_cols = hd23_24[\n",
    "   [\"UNITID\", \"INSTNM\", \"ADDR\", \"CITY\", \"STABBR\",\"ZIP\", \"COUNTYNM\"]\n",
    "   ]\n",
    "print(hd23_24_important_cols.isna().sum())\n",
    "\n",
    "\n",
    "# 3. Demonstrate size of dataset\n",
    "print(\"\\nSize of Datasets\")\n",
    "print(f\"ef23_24: {ef23_24_important_cols.shape}\")\n",
    "print(f\"hd23_24: {hd23_24_important_cols.shape}\")\n",
    "\n",
    "# 4. How much data is missing?\n",
    "print(\"\\n4. No data is missing\")\n",
    "\n",
    "# 5. Suspicious entries?\n",
    "print(\"\\n5. No suspicious entries\")\n",
    "\n",
    "# 6. Demonstrate data is clean\n",
    "print(\"\\n6. Data looks clean\")\n",
    "print(ef23_24_important_cols.head())\n",
    "print(hd23_24_important_cols.head())\n",
    "\n",
    "ef23_24_important_cols.to_csv(\"data/02-processed/EF23-24_Wrangled.csv\")\n",
    "hd23_24_important_cols.to_csv(\"data/02-processed/HD23-24_Wrangled.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: Replace this with your timeline.  **PLEASE UPDATE your Timeline!** No battle plan survives contact with the enemy, so make sure we understand how your plans have changed.  Also if you have lost points on the previous checkpoint fix them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COGS108_FA25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
